<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Residual Off-Policy RL for Finetuning Behavior Cloning Policies - ICRA 2026.">
  <meta name="keywords" content="Reinforcement Learning, Robot Learning, BC and RL">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Residual Off-Policy RL for Finetuning Behavior Cloning Policies</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ”„</text></svg>">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <!-- <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://ankile.com" target="_blank">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://residual-assembly.github.io/" target="_blank">
              ResiP -- Residual Policy Finetuning with PPO
            </a>
            <a class="navbar-item" href="https://diffusion-ppo.github.io" target="_blank">
              DPPO -- Diffusion PPO
            </a>
            <a class="navbar-item" href="https://imitation-juicer.github.io" target="_blank">
              JUICER -- Imitation Learning with Policy Diffusion
            </a>
            <a class="navbar-item" href="https://dexhub.ai/project" target="_blank">
              Dexhub -- Scalable Teleoperation with Augmented Reality
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav> -->


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              Residual Off-Policy RL for Finetuning Behavior Cloning Policies
            </h1>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://ankile.com" target="_blank">Lars Ankile</a><sup>1,2,*</sup>,</span>
              <span class="author-block">
                <a href="https://zhenyujiang.me/" target="_blank">Zhenyu Jiang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="http://rockyduan.com/" target="_blank">Rocky Duan</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.gshi.me/" target="_blank">Guanya Shi</a><sup>1,3,â€ </sup>,</span>
              <span class="author-block">
                <a href="https://people.eecs.berkeley.edu/~pabbeel/" target="_blank">Pieter
                  Abbeel</a><sup>1,4,â€ </sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=DkUUhXEAAAAJ&hl=en" target="_blank">Anusha
                  Nagabandi</a><sup>1</sup></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Amazon FAR (Frontier AI & Robotics)</span>&nbsp;&nbsp;
              <span class="author-block"><sup>2</sup>Stanford University</span>&nbsp;&nbsp;
              <span class="author-block"><sup>3</sup>Carnegie Mellon University</span>&nbsp;&nbsp;
              <span class="author-block"><sup>4</sup>UC Berkeley</span>
            </div>

            <div class="is-size-6 publication-authors" style="margin-top: 0.5em;">
              <span class="author-block"><sup>*</sup>Work done while interning at Amazon FAR</span>&nbsp;&nbsp;
              <span class="author-block"><sup>â€ </sup>Work done while at Amazon FAR</span>
            </div>

            <div class="image-container">
              <!-- <img src="./static/images/MIT_logo.png" alt="MIT Logo" width="12%" style="margin: 1em 0;">
              <img src="./static/images/Shield_CMYK.png" alt="Harvard Logo" width="7%" style="margin: 1em 0;">
              <img src="./static/images/iai-logo-ver_1.jpg" alt="Improbable AI Logo" width="10%" style="margin: 1em 0;"> -->
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a target="_blank" href="" class="external-link button is-normal is-rounded is-dark" disabled>
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper (Coming Soon)</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <!-- <span class="link-block">
                  <a target="_blank" href="https://youtu.be/AsasJJ2jlw8"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>3-Minute Video</span>
                  </a>
                </span> -->
                <!-- Code Link. -->
                <span class="link-block">
                  <a target="_blank" href="" class="external-link button is-normal is-rounded is-dark" disabled>
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming Soon)</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <!-- <span class="link-block">
                  <a href="https://iai-robust-rearrangement.s3.us-east-2.amazonaws.com/index.html"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
                </span> -->
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>




  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/e1OkcOgHY4M?autoplay=1&mute=1&loop=1" frameborder="0"
            allow="autoplay; encrypted-media" allowfullscreen>
          </iframe>
        </div>
        <br>
        <h2 class="subtitle has-text-centered">
          We present ResFiT, a residual RL method that performs real-world RL directly on our 29 Degree-of-Freedom (DoF)
          wheeled humanoid platform with two 5-fingered-hands, demonstrating successful real-world RL training
          on a humanoid robot with dexterous hands.
        </h2>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            Recent advances in behavior cloning (BC) have enabled impressive visuomotor control policies. However, these
            approaches are limited by the quality of human demonstrations, the manual effort required for data
            collection, and the diminishing returns from increasing data.
          </div>
          <div class="content has-text-justified">
            In comparison, reinforcement learning (RL) trains an agent through autonomous interaction with the
            environment and has shown remarkable success in various domains. Still, training RL policies directly on
            real-world robots remains challenging due to sample inefficiency, safety concerns, and the difficulty of
            learning from sparse rewards for long-horizon tasks, especially for high-degree-of-freedom (DoF) systems.
          </div>
          <div class="content has-text-justified">
            We present a recipe that combines the benefits of BC and RL through a residual learning framework. Our
            approach leverages BC policies as black-box bases and learns lightweight per-step residual corrections via
            efficient off-policy RL.
          </div>
          <div class="content has-text-justified">
            We show that our method requires only sparse binary reward signals and can effectively learn manipulation
            tasks on high-DoF systems in both simulation and the real world. In particular, we demonstrate, to the best
            of our knowledge, the first successful real-world RL training on a humanoid robot with dexterous hands.
          </div>
          <div class="content has-text-justified">
            Our results show state-of-the-art performance in various vision-based tasks, establishing a practical
            pathway to deploy RL in the real world.
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video. -->
      <!-- <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <iframe src="https://www.youtube.com/watch?v=AsasJJ2jlw8" frameborder="0" allow="autoplay; encrypted-media"
              allowfullscreen></iframe>
          </div>
        </div>
      </div> -->
      <!--/ Paper video. -->
    </div>
  </section>


  <!-- Summary -->
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">The key insight</h2>

      <div class="content has-text-justified">
        <p>
          Modern BC architectures are typically deep models with tens of millions of parameters that use action chunking
          or diffusion-based approaches, which can make it challenging to apply RL methods to directly optimize the
          policy.
        </p>
        <p>
          Our ResFiT approach leverages BC policies as black-box bases and learns lightweight per-step residual
          corrections
          via efficient off-policy RL, requiring only sparse binary reward signals and enabling real-world training on
          high-DoF systems.
        </p>
      </div>

      <div class="content has-text-centered is-full-width">
        <img src="static/figures/pipeline.svg" alt="Pipeline diagram" style="width: 100%;">
      </div>


      <br><br><br>


      <h2 class="title is-3">Introducing ResFiT: Residual Fine-tuning with Off-Policy RL</h2>

      <div class="content has-text-justified">
        <p>
          ResFiT is a two-phase approach using online RL to improve BC policies. It takes as input a pre-trained BC
          policy
          and learns lightweight per-step residual corrections via efficient off-policy RL.
        </p>
        <p>
          The method leverages BC policies as black-box bases, avoiding the need to modify the complex BC architecture
          while enabling real-world training on high-DoF systems with only sparse binary reward signals.
        </p>
      </div>

      <div class="content has-text-centered is-full-width">
        <img src="static/figures/task_overview.png" alt="Task overview" style="width: 100%;">
      </div>

      <br><br><br>

      <h2 class="title is-3">Residual learning leads to significant performance improvements</h2>

      <div class="content has-text-justified">
        <p>
          Our residual off-policy approach demonstrates substantial improvements in sample efficiency and final
          performance compared to standard off-policy algorithms across a variety of challenging environments.
        </p>

        <p>
          We compare our method to several strong baselines and ablations. First, we examine how off-policy residual RL
          on top of an action-chunked base policy compares to directly performing off-policy RL to learn a single-action
          policy from the same demonstrations. For this comparison, we use an optimized version of RLPD <a
            href="https://arxiv.org/abs/2302.02948" target="_blank">[1]</a>
          (state-of-the-art off-policy RL) called "Tuned RLPD" that incorporates the same design decisions as our method
          but without the base policy. We also compare against IBRL <a href="https://arxiv.org/abs/2311.02198"
            target="_blank">[2]</a>, which uses a pre-trained BC policy to propose
          actions and bootstrap target values during RL training. Additionally, we include "Filtered BC," an online BC
          fine-tuning baseline that starts with the same base policy but iteratively adds successful rollouts back into
          the dataset for continued behavioral cloning rather than using residual RL. Finally, we ablate key design
          choices including layer normalization, demo incorporation during online RL.
        </p>
      </div>

      <img src="static/figures/success_rate_combined.svg" alt="Success rate comparison" style="width: 100%;">


      <br><br><br>
      <h2 class="title is-3">Real-World RL Fine-Tuning using ResFiT</h2>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="content has-text-justified is-full-width">
            <p>We apply the ResFiT method to real-world tasks and demonstrate the first successful real-world RL
              training on a humanoid robot with dexterous hands.
            </p>
          </div>
        </div>
      </div>

      <div class="content has-text-centered">
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/xV3hQNgUiZU?autoplay=1&mute=1&loop=1" frameborder="0"
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
            allowfullscreen></iframe>
        </div>
      </div>

      <br><br><br>
      <h2 class="title is-3">Package Handover Comparisons</h2>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="content has-text-justified is-full-width">
            <p>Side-by-side comparison of base policy (left) vs. residual policy (right) performance on package handover
              tasks.
              The residual policy demonstrates improved success rates and more robust handling across different
              scenarios.
            </p>
          </div>
        </div>
      </div>

      <div class="content has-text-centered">
        <div class="columns is-multiline">
          <!-- Row 1 -->
          <div class="column is-6">
            <h4 class="title is-5">Base Policy</h4>
            <video autoplay loop muted loading="lazy" preload="none" style="width: 100%; border-radius: 10px;">
              <source src="static/videos/package_handover/res-success-base-fail-0-base_compressed_muted.mp4"
                type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-6">
            <h4 class="title is-5">Residual Policy</h4>
            <video autoplay loop muted loading="lazy" preload="none" style="width: 100%; border-radius: 10px;">
              <source src="static/videos/package_handover/res-success-base-fail-0-res_compressed_muted.mp4"
                type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>

          <!-- Row 2 -->
          <div class="column is-6">
            <video autoplay loop muted loading="lazy" preload="none" style="width: 100%; border-radius: 10px;">
              <source src="static/videos/package_handover/res-success-base-fail-1-base_compressed_muted.mp4"
                type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-6">
            <video autoplay loop muted loading="lazy" preload="none" style="width: 100%; border-radius: 10px;">
              <source src="static/videos/package_handover/res-success-base-fail-1-res_compressed_muted.mp4"
                type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>

          <!-- Row 3 -->
          <div class="column is-6">
            <video autoplay loop muted loading="lazy" preload="none" style="width: 100%; border-radius: 10px;">
              <source src="static/videos/package_handover/res-success-base-fail-2-base_compressed_muted.mp4"
                type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-6">
            <video autoplay loop muted loading="lazy" preload="none" style="width: 100%; border-radius: 10px;">
              <source src="static/videos/package_handover/res-success-base-fail-2-res_compressed_muted.mp4"
                type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>

          <!-- Row 4 -->
          <div class="column is-6">
            <video autoplay loop muted loading="lazy" preload="none" style="width: 100%; border-radius: 10px;">
              <source src="static/videos/package_handover/res-success-base-fail-3-base_compressed_muted.mp4"
                type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-6">
            <video autoplay loop muted loading="lazy" preload="none" style="width: 100%; border-radius: 10px;">
              <source src="static/videos/package_handover/res-success-base-fail-3-res_compressed_muted.mp4"
                type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>

          <!-- Row 5 -->
          <div class="column is-6">
            <video autoplay loop muted loading="lazy" preload="none" style="width: 100%; border-radius: 10px;">
              <source src="static/videos/package_handover/res-success-base-fail-4-base_compressed_muted.mp4"
                type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-6">
            <video autoplay loop muted loading="lazy" preload="none" style="width: 100%; border-radius: 10px;">
              <source src="static/videos/package_handover/res-success-base-fail-4-res_compressed_muted.mp4"
                type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
      </div>

      <br><br><br>
      <h2 class="title is-3">What Matters for Performance?</h2>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="content has-text-justified is-full-width">
            <p>We find that in long-horizon tasks with sparse rewards, the Update-to-Data (UTD) ratio and using n-step
              returns to be crucial.
            </p>
          </div>
        </div>
      </div>

      <div class="content has-text-centered">
        <img src="static/figures/success_rate_combined_utd_ablations.svg" alt="UTD ratio ablation study"
          style="width: 100%; margin-bottom: 20px;">
        <img src="static/figures/success_rate_combined_nstep_ablations.svg" alt="N-step returns ablation study"
          style="width: 100%;">
      </div>


    </div>
  </section>









  <section class="section">
    <div class="container is-max-desktop">


      <!-- Concurrent Work. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Related Links</h2>
          <div class="content has-text-justified">
            <p>
              There's a lot of excellent work related to ours in the space of manipulation and assembly, reinforcement
              learning, and diffusion models. Here are some notable examples:
            </p>

            <h3>Policy Learning and Fine-tuning</h3>
            <ul>
              <li><a target="_blank" href="https://arxiv.org/abs/2412.06685">Policy-Agnostic Reinforcement Learning</a>
                introduces a universal method for fine-tuning various policy classes, including diffusion and
                transformer-based policies, by decoupling policy improvement from specific policy architectures to
                enhance sample efficiency in both offline and online RL settings.</li>
              <li><a target="_blank" href="https://arxiv.org/abs/2007.04976">Policy Decorator</a> presents a modular
                approach that allows a single policy to generalize across diverse agent morphologies, enabling control
                over various agents with different skeletal structures through shared modular policies.</li>
              <li><a target="_blank" href="https://arxiv.org/abs/2412.06685">EXPO: Expressive Policy Fine-tuning</a>
                focuses on refining pre-trained policies by leveraging expressive models to adapt and improve
                performance in new tasks or environments through efficient policy adaptation.</li>
              <li><a target="_blank" href="https://arxiv.org/abs/2507.07969">Q-chunking</a> presents a simple yet
                effective recipe for improving reinforcement learning algorithms for long-horizon, sparse-reward tasks
                by applying action chunking to temporal difference-based RL methods, enabling agents to leverage
                temporally consistent behaviors from offline data for more effective online exploration.</li>
              <li><a target="_blank" href="https://arxiv.org/abs/2506.04168">Horizon Reduction Makes RL Scalable</a>
                studies the scalability of offline reinforcement learning algorithms and introduces SHARSA, a minimal
                yet scalable method that effectively reduces the horizon to unlock the scalability of offline RL on
                challenging tasks.</li>
            </ul>

            <h3>Manipulation and Assembly</h3>
            <ul>
              <li><a target="_blank" href="https://clvrai.com/furniture-bench">FurnitureBench</a> introduces a
                real-world furniture
                assembly benchmark, providing a reproducible and easy-to-use platform for long-horizon complex robotic
                manipulation that we use in our work.</li>
              <li><a target="_blank" href="https://arxiv.org/abs/2309.16909">ASAP</a> is a physics-based planning
                approach for
                automatically generating sequences for general-shaped assemblies, accounting for gravity to design a
                sequence where each sub-assembly is physically stable.</li>
              <li><a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/9645269">InsertionNet 1.0</a>
                and <a href="https://ieeexplore.ieee.org/abstract/document/10016821">InsertionNet 2.0</a> address the
                problem
                of insertion specifically and propose regression-based methods that combine visual and force inputs to
                solve various insertion tasks efficiently and robustly.</li>
              <li><a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/9561662">Grasping with
                  Chopsticks</a> develops
                an autonomous chopsticks-equipped robotic manipulator for picking up small objects, using approaches to
                reduce covariate shift and improve generalization.</li>
            </ul>

            <h3>Diffusion Models and Reinforcement Learning</h3>
            <p>
              Recent work has explored combining diffusion models with reinforcement learning:
            </p>
            <ul>
              <li><a target="_blank" href="http://rl-diffusion.github.io/">Black et al.</a> and <a
                  href="https://arxiv.org/abs/2305.16381">Fan et al.</a> studied how to cast diffusion de-noising as a
                Markov Decision Process, enabling preference-aligned image generation with policy gradient RL.</li>
              <li><a target="_blank" href="https://arxiv.org/abs/2304.10573">IDQL</a> uses a Q-function to select the
                best among
                multiple diffusion model outputs.</li>
              <li><a target="_blank" href="https://arxiv.org/abs/2206.00695">Goo et al.</a> explored advantage weighted
                regression for
                diffusion models.</li>
              <li><a target="_blank" href="https://anuragajay.github.io/decision-diffuser/">Decision
                  Diffuser</a> and related works
                change the
                objective into a supervised learning problem with return conditioning.</li>
              <li><a target="_blank" href="https://arxiv.org/abs/2208.06193">Wang et al.</a> explored augmenting the
                de-noising training
                objective with a Q-function maximization objective.</li>
            </ul>

            <h3>Residual Learning in Robotics</h3>
            <p>
              Learning corrective residual components has seen widespread success in robotics:
            </p>
            <ul>
              <li>Works like <a target="_blank" href="https://k-r-allen.github.io/residual-policy-learning/">Silver et
                  al.</a>, <a target="_blank" href="https://arxiv.org/abs/2112.06749">Davchev et al.</a>, and others
                have explored learning residual policies that correct for errors made by a nominal behavior policy.</li>
              <li><a target="_blank" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8593995">Ajay et al.</a>
                and <a target="_blank" href="https://journals.sagepub.com/doi/full/10.1177/0278364920954896">Kloss et
                  al.</a> combined learned components to correct for inaccuracies in analytical models for physical
                dynamics.</li>
              <li><a target="_blank" href="https://industrial-insertion-rl.github.io/">Schoettler et al.</a> applied
                residual policies to insertion tasks.</li>
              <li><a target="_blank" href="https://transic-robot.github.io/"></a>TRANSIC by Jiang et al.</a> applied
                residual policy learning to the FurnitureBench task suite, using the residual component to model online
                human-provided corrections.</li>
            </ul>

            <h3>Theoretical Analysis of Imitation Learning</h3>
            <p>
              There's been an increasing amount of theoretical analysis of imitation learning, with recent works
              focusing on the properties of noise injection and corrective actions:
            </p>
            <ul>
              <li><a target="_blank" href="https://arxiv.org/abs/2307.14619">Provable Guarantees for Generative Behavior
                  Cloning</a>
                proposes a framework for generative behavior cloning, ensuring continuity through data augmentation and
                noise injection.</li>
              <li><a target="_blank" href="https://arxiv.org/abs/2310.12972">CCIL</a> generates corrective data using
                local continuity
                in environment dynamics.</li>
              <li><a target="_blank" href="https://arxiv.org/abs/2205.14812">TaSIL</a> penalizes deviations in
                higher-order Taylor
                series terms between learned and expert policies.</li>
            </ul>
            <p>
              These works aim to enhance the robustness and sample efficiency of imitation learning algorithms.
            </p>
          </div>
        </div>
      </div>
      <!--/ Concurrent Work. -->

      <h2 class="title is-3">Future Directions</h2>

      <div class="content has-text-justified">
        <p>
          Exciting future directions: Our method is base-model agnostic - we show it works with diffusion models, ACT,
          and MLPs!
        </p>
        <p>
          This means it could potentially scale to fine-tuning large multi-task behavior models (like Octo, OpenVLA,
          etc.) while fully preserving their pre-training capabilities.
        </p>
      </div>

    </div>
  </section>



  </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{ankile2025residualoffpolicy,
        title={Residual Off-Policy RL for Finetuning Behavior Cloning Policies}, 
        author={Lars Ankile and Zhenyu Jiang and Rocky Duan and Guanya Shi and Pieter Abbeel and Anusha Nagabandi},
        year={2026},
        primaryClass={cs.RO}
  }
</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://github.com/ankile" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website template is adapted from <a
                href="https://github.com/residual-assembly/residual-assembly.github.io">Residual Assembly</a>, which was
              adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>