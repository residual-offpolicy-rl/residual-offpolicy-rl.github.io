<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Residual Off-Policy RL for Finetuning Behavior Cloning Policies - ICRA 2026.">
  <meta name="keywords" content="Reinforcement Learning, Robot Learning, BC and RL">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Residual Off-Policy RL for Finetuning Behavior Cloning Policies</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ”„</text></svg>">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              Residual Off-Policy RL for Finetuning Behavior Cloning Policies
            </h1>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://ankile.com" target="_blank">Lars Ankile</a><sup>*1,2</sup>,</span>
              <span class="author-block">
                <a href="https://zhenyujiang.me/" target="_blank">Zhenyu Jiang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="http://rockyduan.com/" target="_blank">Rocky Duan</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.gshi.me/" target="_blank">Guanya Shi</a><sup>â€ 1,3</sup>,</span>
              <span class="author-block">
                <a href="https://people.eecs.berkeley.edu/~pabbeel/" target="_blank">Pieter
                  Abbeel</a><sup>â€ 1,4</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=DkUUhXEAAAAJ&hl=en" target="_blank">Anusha
                  Nagabandi</a><sup>1</sup></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Amazon FAR (Frontier AI & Robotics)</span>&nbsp;&nbsp;
              <span class="author-block"><sup>2</sup>Stanford University</span>&nbsp;&nbsp;
              <span class="author-block"><sup>3</sup>Carnegie Mellon University</span>&nbsp;&nbsp;
              <span class="author-block"><sup>4</sup>UC Berkeley</span>
            </div>

            <div class="is-size-6 publication-authors" style="margin-top: 0.5em;">
              <span class="author-block"><sup>*</sup>Work done while interning at Amazon FAR</span>&nbsp;&nbsp;
              <span class="author-block"><sup>â€ </sup>Work done while at Amazon FAR</span>
            </div>


            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a target="_blank" href="" class="external-link button is-normal is-rounded is-dark" disabled>
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper (Coming Soon)</span>
                  </a>
                </span>
                &nbsp;&nbsp;&nbsp;&nbsp;
                <span class="link-block">
                  <a target="_blank" href="" class="external-link button is-normal is-rounded is-dark" disabled>
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming Soon)</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>




  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/e1OkcOgHY4M?autoplay=1&mute=1&loop=1" frameborder="0"
            allow="autoplay; encrypted-media" allowfullscreen>
          </iframe>
        </div>
        <br>
        <h2 class="subtitle has-text-centered">
          We present ResFiT, a residual RL method that performs real-world RL directly on our 29 Degree-of-Freedom (DoF)
          wheeled humanoid platform with two 5-fingered-hands, demonstrating successful real-world RL training
          on a humanoid robot with dexterous hands.
        </h2>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            Recent advances in behavior cloning (BC) have enabled impressive visuomotor control policies. However, these
            approaches are limited by the quality of human demonstrations, the manual effort required for data
            collection, and the diminishing returns from increasing data.
          </div>
          <div class="content has-text-justified">
            In comparison, reinforcement learning (RL) trains an agent through autonomous interaction with the
            environment and has shown remarkable success in various domains. Still, training RL policies directly on
            real-world robots remains challenging due to sample inefficiency, safety concerns, and the difficulty of
            learning from sparse rewards for long-horizon tasks, especially for high-degree-of-freedom (DoF) systems.
          </div>
          <div class="content has-text-justified">
            We present a recipe that combines the benefits of BC and RL through a residual learning framework. Our
            approach leverages BC policies as black-box bases and learns lightweight per-step residual corrections via
            efficient off-policy RL.
          </div>
          <div class="content has-text-justified">
            We show that our method requires only sparse binary reward signals and can effectively learn manipulation
            tasks on high-DoF systems in both simulation and the real world. In particular, we demonstrate, to the best
            of our knowledge, the first successful real-world RL training on a humanoid robot with dexterous hands.
          </div>
          <div class="content has-text-justified">
            Our results show state-of-the-art performance in various vision-based tasks, establishing a practical
            pathway to deploy RL in the real world.
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

    </div>
  </section>


  <!-- Summary -->
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">The key insight</h2>

      <div class="content has-text-justified">
        <p>
          Modern BC architectures are typically deep models with tens of millions of parameters that use action chunking
          or diffusion-based approaches, which can make it challenging to apply RL methods to directly optimize the
          policy.
        </p>
        <p>
          Our ResFiT approach leverages BC policies as black-box bases and learns lightweight per-step residual
          corrections
          via efficient off-policy RL, requiring only sparse binary reward signals and enabling real-world training on
          high-DoF systems.
        </p>
      </div>

      <div class="content has-text-centered is-full-width">
        <img src="static/figures/pipeline.svg" alt="Pipeline diagram" style="width: 100%;">
      </div>


      <br><br><br>


      <h2 class="title is-3">Introducing ResFiT: Residual Fine-tuning with Off-Policy RL</h2>

      <div class="content has-text-justified">
        <p>
          ResFiT is a two-phase approach using online RL to improve BC policies. It takes as input a pre-trained BC
          policy
          and learns lightweight per-step residual corrections via efficient off-policy RL.
        </p>
        <p>
          The method leverages BC policies as black-box bases, avoiding the need to modify the complex BC architecture
          while enabling real-world training on high-DoF systems with only sparse binary reward signals.
        </p>
      </div>

      <div class="content has-text-centered is-full-width">
        <img src="static/figures/task_overview.png" alt="Task overview" style="width: 100%;">
      </div>

      <br><br><br>

      <h2 class="title is-3">Residual learning leads to significant performance improvements</h2>

      <div class="content has-text-justified">
        <p>
          Our residual off-policy approach demonstrates substantial improvements in sample efficiency and final
          performance compared to standard off-policy algorithms across a variety of challenging environments.
        </p>

        <p>
          We compare our method to several strong baselines and ablations. First, we examine how off-policy residual RL
          on top of an action-chunked base policy compares to directly performing off-policy RL to learn a single-action
          policy from the same demonstrations. For this comparison, we use an optimized version of RLPD <a
            href="https://arxiv.org/abs/2302.02948" target="_blank">[1]</a>
          (state-of-the-art off-policy RL) called "Tuned RLPD" that incorporates the same design decisions as our method
          but without the base policy. We also compare against IBRL <a href="https://arxiv.org/abs/2311.02198"
            target="_blank">[2]</a>, which uses a pre-trained BC policy to propose
          actions and bootstrap target values during RL training. Additionally, we include "Filtered BC," an online BC
          fine-tuning baseline that starts with the same base policy but iteratively adds successful rollouts back into
          the dataset for continued behavioral cloning rather than using residual RL. Finally, we ablate key design
          choices including layer normalization, demo incorporation during online RL.
        </p>
      </div>

      <img src="static/figures/success_rate_combined.svg" alt="Success rate comparison" style="width: 100%;">
      <p class="has-text-grey-light" style="font-size: 0.9em; margin-top: 10px; text-align: center;">
        Performance comparison across simulation tasks showing ResFiT's improvements over baseline methods including
        Tuned RLPD, IBRL, and Filtered BC.
      </p>





      <br><br><br>
      <h2 class="title is-3">Off-Policy vs On-Policy Residual RL</h2>

      <div class="columns is-centered is-vcentered">
        <div class="column is-half">
          <div class="content has-text-justified"
            style="display: flex; flex-direction: column; justify-content: center; height: 100%;">
            <div>
              <p>
                On the simulated <strong>BoxClean</strong> task, we compare our off-policy residual RL approach to
                performing residual RL with the on-policy PPO algorithm, as done in the ResiP algorithm <a
                  href="https://arxiv.org/pdf/2407.16677" target="_blank">[3]</a>.
              </p>
              <p>
                Our approach converges at 200k steps versus 40M steps, demonstrating a <strong>200Ã— boost in sample
                  efficiency</strong>. This improvement highlights the need for off-policy approaches when performing
                RL directly in the real world.
              </p>
            </div>
          </div>
        </div>
        <div class="column is-half">
          <div class="content has-text-centered"
            style="display: flex; align-items: center; justify-content: center; height: 100%;">
            <img src="static/figures/td3_vs_ppo_boxclean.svg" alt="Off-policy vs On-policy comparison on BoxClean task"
              style="width: 100%; max-width: 100%;">
            <p class="has-text-grey-light" style="font-size: 0.9em; margin-top: 10px; text-align: center;">
              Off-policy TD3 converges in 200k steps vs. on-policy PPO requiring 40M stepsâ€”a 200Ã— improvement in sample
              efficiency.
            </p>
          </div>
        </div>
      </div>

      <br><br><br>
      <h2 class="title is-3">Real-World RL Fine-Tuning using ResFiT</h2>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="content has-text-justified is-full-width">
            <p>We apply the ResFiT method to real-world tasks and demonstrate, to our knowledge, the first successful
              real-world RL
              training on a humanoid robot with dexterous hands.
            </p>
          </div>
        </div>
      </div>

      <div class="content has-text-centered">
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/xV3hQNgUiZU?autoplay=1&mute=1&loop=1" frameborder="0"
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
            allowfullscreen></iframe>
        </div>
      </div>

      <br><br><br>
      <h2 class="title is-3">Real-World Task Results</h2>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="content has-text-justified is-full-width">
            <p>We demonstrate ResFiT on a real-world bimanual dexterous manipulation platform - the wheeled Vega
              humanoid
              from Dexmate. The robot features two 7-DoF arms, two 6-DoF OyMotion dexterous hands, and 2 image streams
              from a head-mounted Zed camera, with a 29-dimensional action space using absolute joint position control.
            </p>
            <p><strong>Tasks:</strong> We evaluate on two challenging manipulation tasks. In
              <code>WoollyBallPnP</code>,
              the right hand picks up a ball from a random table location and places it into a randomly positioned tote.
              In <code>PackageHandover</code>, the right hand picks up a deformable package, hands it to the left
              hand,
              and places it into a tote on the left side of the workspace.
            </p>
            <p><strong>Evaluation:</strong> To ensure fair comparison, we use blind A/B testing with matched initial
              conditions. For each round, we sample random scene configurations, randomly assign policies to labels A
              and B,
              execute both policies from identical states, and reveal identities only after completion. This mitigates
              evaluator bias and environmental confounds common in real-world robot evaluations.
            </p>
          </div>
        </div>
      </div>

      <div class="content has-text-centered">
        <img src="static/figures/real_world_result.png" alt="Real-world task success rates comparison"
          style="max-width: 100%; height: auto;">
        <p class="has-text-grey-light" style="font-size: 0.9em; margin-top: 10px;">
          Success rates on real-world tasks comparing BC with ACT baseline vs. ResFiT residual policy.
        </p>
      </div>

      <br><br><br>
      <h2 class="title is-3"><code>PackageHandover</code> Comparisons</h2>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="content has-text-justified is-full-width">
            <p>Side-by-side comparison of base policy (left) vs. residual policy (right) performance on package handover
              tasks.
              The residual policy demonstrates improved success rates and more robust handling across different
              scenarios.
            </p>
          </div>
        </div>
      </div>

      <div class="content has-text-centered">
        <div class="columns is-multiline">
          <!-- Row 1 -->
          <div class="column is-6">
            <h4 class="title is-5">Base Policy</h4>
            <div class="video-container-4-3">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/package_handover/res-success-base-fail-0-base.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
          <div class="column is-6">
            <h4 class="title is-5">Residual Policy</h4>
            <div class="video-container-4-3">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/package_handover/res-success-base-fail-0-res.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>

          <!-- Row 2 -->
          <div class="column is-6">
            <div class="video-container-4-3">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/package_handover/res-success-base-fail-1-base.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
          <div class="column is-6">
            <div class="video-container-4-3">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/package_handover/res-success-base-fail-1-res.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>

          <!-- Row 3 -->
          <div class="column is-6">
            <div class="video-container-4-3">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/package_handover/res-success-base-fail-2-base.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
          <div class="column is-6">
            <div class="video-container-4-3">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/package_handover/res-success-base-fail-2-res.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>

          <!-- Row 4 -->
          <div class="column is-6">
            <div class="video-container-4-3">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/package_handover/res-success-base-fail-3-base.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
          <div class="column is-6">
            <div class="video-container-4-3">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/package_handover/res-success-base-fail-3-res.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>

          <!-- Row 5 -->
          <div class="column is-6">
            <div class="video-container-4-3">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/package_handover/res-success-base-fail-4-base.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
          <div class="column is-6">
            <div class="video-container-4-3">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/package_handover/res-success-base-fail-4-res.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
        </div>
      </div>

      <br><br><br>
      <h2 class="title is-3"><code>WoollyBallPnP</code> Comparisons</h2>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="content has-text-justified is-full-width">
            <p>Side-by-side comparison of base policy (left) vs. residual policy (right) performance on woolly ball
              manipulation tasks.
              These examples showcase scenarios where the base policy fails but the residual policy successfully
              completes the task, demonstrating the robustness improvements achieved through residual learning.
            </p>
          </div>
        </div>
      </div>

      <div class="content has-text-centered">
        <div class="columns is-multiline">
          <!-- Row 1 -->
          <div class="column is-6">
            <h4 class="title is-5">Base Policy</h4>
            <div class="video-container-4-3 shift-right">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/woolly_ball/woollyball-bc-fail-rl-success-0-base.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
          <div class="column is-6">
            <h4 class="title is-5">Residual Policy</h4>
            <div class="video-container-4-3 shift-right">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/woolly_ball/woollyball-bc-fail-rl-success-0-rl.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>

          <!-- Row 2 -->
          <div class="column is-6">
            <div class="video-container-4-3 shift-right">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/woolly_ball/woollyball-bc-fail-rl-success-1-base.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
          <div class="column is-6">
            <div class="video-container-4-3 shift-right">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/woolly_ball/woollyball-bc-fail-rl-success-1-rl.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>

          <!-- Row 3 -->
          <div class="column is-6">
            <div class="video-container-4-3 shift-right">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/woolly_ball/woollyball-bc-fail-rl-success-2-base.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
          <div class="column is-6">
            <div class="video-container-4-3 shift-right">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/woolly_ball/woollyball-bc-fail-rl-success-2-rl.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>

          <!-- Row 4 -->
          <div class="column is-6">
            <div class="video-container-4-3 shift-right">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/woolly_ball/woollyball-bc-fail-rl-success-3-base.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
          <div class="column is-6">
            <div class="video-container-4-3 shift-right">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/woolly_ball/woollyball-bc-fail-rl-success-3-rl.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>

          <!-- Row 5 -->
          <div class="column is-6">
            <div class="video-container-4-3 shift-right">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/woolly_ball/woollyball-bc-fail-rl-success-4-base.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
          <div class="column is-6">
            <div class="video-container-4-3 shift-right">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/woolly_ball/woollyball-bc-fail-rl-success-4-rl.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>

          <!-- Row 6 -->
          <div class="column is-6">
            <div class="video-container-4-3 shift-right">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/woolly_ball/woollyball-bc-fail-rl-success-5-base.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
          <div class="column is-6">
            <div class="video-container-4-3 shift-right">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/woolly_ball/woollyball-bc-fail-rl-success-5-rl.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>

          <!-- Row 7 -->
          <div class="column is-6">
            <div class="video-container-4-3 shift-right">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/woolly_ball/woollyball-bc-fail-rl-success-6-base.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
          <div class="column is-6">
            <div class="video-container-4-3 shift-right">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/woolly_ball/woollyball-bc-fail-rl-success-6-rl.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
        </div>
      </div>

      <br><br><br>
      <h2 class="title is-3">What Matters for Performance?</h2>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="content has-text-justified is-full-width">
            <p>We find that in long-horizon tasks with sparse rewards, the Update-to-Data (UTD) ratio and using n-step
              returns to be crucial.
            </p>
          </div>
        </div>
      </div>

      <div class="content has-text-centered">
        <img src="static/figures/success_rate_combined_utd_ablations.svg" alt="UTD ratio ablation study"
          style="width: 100%; margin-bottom: 10px;">
        <p class="has-text-grey-light" style="font-size: 0.9em; margin-top: 10px;">
          UTD ratio ablation showing optimal performance at UTD=4, with diminishing returns at higher values.
        </p>
      </div>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="content has-text-justified is-full-width">
            <p>
              For <strong>UTD ratios</strong>, we observe that for tasks with horizon lengths of 150-250 steps (such as
              BoxCleanup),
              UTD values greater than 1 provide clear benefits, but gains plateau at moderate values. Learning is
              noticeably
              slower for a UTD of Â½, but increasing to very high UTDs of 8 or higher yields diminishing returns, with
              UTDs
              of 4 already capturing most of the benefit while remaining stable.
            </p>
          </div>
        </div>
      </div>

      <div class="content has-text-centered">
        <img src="static/figures/success_rate_combined_nstep_ablations.svg" alt="N-step returns ablation study"
          style="width: 100%; margin-bottom: 10px;">
        <p class="has-text-grey-light" style="font-size: 0.9em; margin-top: 10px;">
          N-step returns ablation demonstrating the importance of multi-step returns for sparse reward tasks, with
          optimal performance around n=5.
        </p>
      </div>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="content has-text-justified is-full-width">
            <p>
              For <strong>n-step returns</strong>, we see the importance of using values larger than 1 for sparse reward
              tasks.
              However, since larger n-step values also increase bias, excessively large values can negatively affect
              performance,
              requiring a careful balance between reducing variance and controlling bias.
            </p>
          </div>
        </div>
      </div>


      <br><br><br>
      <h2 class="title is-3">Comparison with Baseline in Simulation</h2>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="content has-text-justified is-full-width">
            <p>Side-by-side comparison of BC policy (left) vs. Tuned RLPD (center) vs. ResFiT (right) performance on
              simulated tasks. Tuned RLPD shows much faster but very different behavior than the BC policy, and it fails
              on the coffee task. ResFiT improves performance over all tasks.
            </p>
          </div>
        </div>
      </div>

      <div class="content has-text-centered">
        <div class="columns is-multiline">
          <!-- Row 1 -->
          <div class="column is-4">
            <h4 class="title is-5">BC Policy</h4>
            <div class="video-container-square">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/simulation/coffee-bc_540p.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
            <p>Success Rate: 77% <br> Average Number of Steps: 322</p>
          </div>
          <div class="column is-4">
            <h4 class="title is-5">Tuned RLPD</h4>
            <div class="video-container-square">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/simulation/coffee-rlpd_540p.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
            <p>Success Rate: 0% <br> Average Number of Steps: N/A</p>
          </div>
          <div class="column is-4">
            <h4 class="title is-5">ResFiT</h4>
            <div class="video-container-square">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/simulation/coffee-res_540p.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
            <p>Success Rate: 98% <br> Average Number of Steps: 141</p>
          </div>

          <!-- Row 2 -->
          <div class="column is-4">
            <div class="video-container-square">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/simulation/box-bc_540p.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
            <p>Success Rate: 76% <br> Average Number of Steps: 231</p>
          </div>
          <div class="column is-4">
            <div class="video-container-square">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/simulation/box-rlpd_540p.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
            <p>Success Rate: 98% <br> Average Number of Steps: 39</p>
          </div>
          <div class="column is-4">
            <div class="video-container-square">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/simulation/box-res_540p.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
            <p>Success Rate: 97% <br> Average Number of Steps: 97</p>
          </div>

          <!-- Row 3 -->
          <div class="column is-4">
            <div class="video-container-square">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/simulation/cansort-bc_540p.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
            <p>Success Rate: 89% <br> Average Number of Steps: 292</p>
          </div>
          <div class="column is-4">
            <div class="video-container-square">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/simulation/cansort-rlpd_540p.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
            <p>Success Rate: 93% <br> Average Number of Steps: 27</p>
          </div>
          <div class="column is-4">
            <div class="video-container-square">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/simulation/cansort-res_540p.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
            <p>Success Rate: 99% <br> Average Number of Steps: 166</p>
          </div>

          <!-- Row 4 -->
          <div class="column is-4">
            <div class="video-container-square">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/simulation/square-bc_540p.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
            <p>Success Rate: 76% <br> Average Number of Steps: 174</p>
          </div>
          <div class="column is-4">
            <div class="video-container-square">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/simulation/square-rlpd_540p.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
            <p>Success Rate: 99% <br> Average Number of Steps: 39</p>
          </div>
          <div class="column is-4">
            <div class="video-container-square">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/simulation/square-res_540p.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
            <p>Success Rate: 91% <br> Average Number of Steps: 121</p>
          </div>

          <!-- Row 5 -->
          <div class="column is-4">
            <div class="video-container-square">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/simulation/can-bc_540p.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
            <p>Success Rate: 78% <br> Average Number of Steps: 160</p>
          </div>
          <div class="column is-4">
            <div class="video-container-square">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/simulation/can-rlpd_540p.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
            <p>Success Rate: 97% <br> Average Number of Steps: 41</p>
          </div>
          <div class="column is-4">
            <div class="video-container-square">
              <video autoplay loop muted loading="lazy" preload="none">
                <source src="static/videos/simulation/can-res_540p.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
            <p>Success Rate: 99% <br> Average Number of Steps: 110</p>
          </div>

        </div>
      </div>


    </div>
  </section>









  <section class="section">
    <div class="container is-max-desktop">


      <!-- Concurrent Work. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Related Links</h2>
          <div class="content has-text-justified">
            <p>
              There's a lot of excellent work related to ours in the space of manipulation and assembly, reinforcement
              learning, and diffusion models. Here are some notable examples:
            </p>

            <h3>Policy Learning and Fine-tuning</h3>
            <ul>
              <li><a target="_blank" href="https://arxiv.org/abs/2412.06685">Policy-Agnostic Reinforcement Learning</a>
                introduces a universal method for fine-tuning various policy classes, including diffusion and
                transformer-based policies, by decoupling policy improvement from specific policy architectures to
                enhance sample efficiency in both offline and online RL settings.</li>
              <li><a target="_blank" href="https://arxiv.org/abs/2007.04976">Policy Decorator</a> presents a modular
                approach that allows a single policy to generalize across diverse agent morphologies, enabling control
                over various agents with different skeletal structures through shared modular policies.</li>
              <li><a target="_blank" href="https://arxiv.org/abs/2412.06685">EXPO: Expressive Policy Fine-tuning</a>
                focuses on refining pre-trained policies by leveraging expressive models to adapt and improve
                performance in new tasks or environments through efficient policy adaptation.</li>
              <li><a target="_blank" href="https://arxiv.org/abs/2507.07969">Q-chunking</a> presents a simple yet
                effective recipe for improving reinforcement learning algorithms for long-horizon, sparse-reward tasks
                by applying action chunking to temporal difference-based RL methods, enabling agents to leverage
                temporally consistent behaviors from offline data for more effective online exploration.</li>
              <li><a target="_blank" href="https://arxiv.org/abs/2506.04168">Horizon Reduction Makes RL Scalable</a>
                studies the scalability of offline reinforcement learning algorithms and introduces SHARSA, a minimal
                yet scalable method that effectively reduces the horizon to unlock the scalability of offline RL on
                challenging tasks.</li>
            </ul>

            <h3>Manipulation and Assembly</h3>
            <ul>
              <li><a target="_blank" href="https://clvrai.com/furniture-bench">FurnitureBench</a> introduces a
                real-world furniture
                assembly benchmark, providing a reproducible and easy-to-use platform for long-horizon complex robotic
                manipulation that we use in our work.</li>
              <li><a target="_blank" href="https://arxiv.org/abs/2309.16909">ASAP</a> is a physics-based planning
                approach for
                automatically generating sequences for general-shaped assemblies, accounting for gravity to design a
                sequence where each sub-assembly is physically stable.</li>
              <li><a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/9645269">InsertionNet 1.0</a>
                and <a href="https://ieeexplore.ieee.org/abstract/document/10016821">InsertionNet 2.0</a> address the
                problem
                of insertion specifically and propose regression-based methods that combine visual and force inputs to
                solve various insertion tasks efficiently and robustly.</li>
              <li><a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/9561662">Grasping with
                  Chopsticks</a> develops
                an autonomous chopsticks-equipped robotic manipulator for picking up small objects, using approaches to
                reduce covariate shift and improve generalization.</li>
            </ul>

            <h3>Diffusion Models and Reinforcement Learning</h3>
            <p>
              Recent work has explored combining diffusion models with reinforcement learning:
            </p>
            <ul>
              <li><a target="_blank" href="http://rl-diffusion.github.io/">Black et al.</a> and <a
                  href="https://arxiv.org/abs/2305.16381">Fan et al.</a> studied how to cast diffusion de-noising as a
                Markov Decision Process, enabling preference-aligned image generation with policy gradient RL.</li>
              <li><a target="_blank" href="https://arxiv.org/abs/2304.10573">IDQL</a> uses a Q-function to select the
                best among
                multiple diffusion model outputs.</li>
              <li><a target="_blank" href="https://arxiv.org/abs/2206.00695">Goo et al.</a> explored advantage weighted
                regression for
                diffusion models.</li>
              <li><a target="_blank" href="https://anuragajay.github.io/decision-diffuser/">Decision
                  Diffuser</a> and related works
                change the
                objective into a supervised learning problem with return conditioning.</li>
              <li><a target="_blank" href="https://arxiv.org/abs/2208.06193">Wang et al.</a> explored augmenting the
                de-noising training
                objective with a Q-function maximization objective.</li>
            </ul>

            <h3>Residual Learning in Robotics</h3>
            <p>
              Learning corrective residual components has seen widespread success in robotics:
            </p>
            <ul>
              <li>Works like <a target="_blank" href="https://k-r-allen.github.io/residual-policy-learning/">Silver et
                  al.</a>, <a target="_blank" href="https://arxiv.org/abs/2112.06749">Davchev et al.</a>, and others
                have explored learning residual policies that correct for errors made by a nominal behavior policy.</li>
              <li><a target="_blank" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8593995">Ajay et al.</a>
                and <a target="_blank" href="https://journals.sagepub.com/doi/full/10.1177/0278364920954896">Kloss et
                  al.</a> combined learned components to correct for inaccuracies in analytical models for physical
                dynamics.</li>
              <li><a target="_blank" href="https://industrial-insertion-rl.github.io/">Schoettler et al.</a> applied
                residual policies to insertion tasks.</li>
              <li><a target="_blank" href="https://transic-robot.github.io/"></a>TRANSIC by Jiang et al.</a> applied
                residual policy learning to the FurnitureBench task suite, using the residual component to model online
                human-provided corrections.</li>
            </ul>

            <h3>Theoretical Analysis of Imitation Learning</h3>
            <p>
              There's been an increasing amount of theoretical analysis of imitation learning, with recent works
              focusing on the properties of noise injection and corrective actions:
            </p>
            <ul>
              <li><a target="_blank" href="https://arxiv.org/abs/2307.14619">Provable Guarantees for Generative Behavior
                  Cloning</a>
                proposes a framework for generative behavior cloning, ensuring continuity through data augmentation and
                noise injection.</li>
              <li><a target="_blank" href="https://arxiv.org/abs/2310.12972">CCIL</a> generates corrective data using
                local continuity
                in environment dynamics.</li>
              <li><a target="_blank" href="https://arxiv.org/abs/2205.14812">TaSIL</a> penalizes deviations in
                higher-order Taylor
                series terms between learned and expert policies.</li>
            </ul>
            <p>
              These works aim to enhance the robustness and sample efficiency of imitation learning algorithms.
            </p>
          </div>
        </div>
      </div>
      <!--/ Concurrent Work. -->

      <h2 class="title is-3">Future Directions</h2>

      <div class="content has-text-justified">
        <p>
          Looking ahead, figuring out the right way to relax the frozen base constraint without sacrificing stability
          could provide further improvement in performance and robustness. If we can distill the more precise, reliable,
          and fast behavior from the combined policy back into the base policy, that would provide more room for the
          residual model to improve further.
        </p>
        <p>
          This would be particularly powerful in the multitask setting, where one can distill task-specific residual
          improvements into an increasingly capable generalist. Our method is base-model agnostic and could potentially
          scale to fine-tuning large multi-task behavior models while fully preserving their pre-training capabilities.
        </p>
      </div>

    </div>
  </section>



  </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{ankile2025residualoffpolicy,
        title={Residual Off-Policy RL for Finetuning Behavior Cloning Policies}, 
        author={Lars Ankile and Zhenyu Jiang and Rocky Duan and Guanya Shi and Pieter Abbeel and Anusha Nagabandi},
        year={2026},
        primaryClass={cs.RO}
  }
</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://github.com/ankile" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website template is adapted from <a
                href="https://github.com/residual-assembly/residual-assembly.github.io">Residual Assembly</a>, which was
              adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>